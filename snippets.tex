






\subsection{Theory motivation}


\paragraph{Higgs Xsec WG}

\subsection{Choice of parameters}

\subsubsection{Choices in {[}1{]}}

\subsubsection{Vacuum stability study}

\paragraph{In all scenarios, the lambda parameters are fixed to 3 to
ensure vacuum stability.}

\subsection{Parameter scans on masses, couplings and mixing angles}

\subsubsection{Logic of how one proceeds}

\paragraph{Starting from one of the benchmarks in {[}1{]} (benchmark 3)}

\paragraph{Mapping the kinematics and sensitivity of the model in the
various parameters}

\subparagraph{Can we rescale existing DMF samples? Yes for DM+ttbar}

\subsubsection{Results for different signatures:}

Each of the signatures should have the following plots in the planes
below: - efficiency at parton level with simplified, published cuts -
total and fiducial cross-section at parton level - 2 - 3 kinematic plots
that are most representative (let the analysers decide, then harmonize)

\paragraph{Mono-Z (lep/had)}

\paragraph{MonoH-\textgreater{}bb}

\paragraph{Monojet}

\paragraph{ttbar+MET}

\subparagraph{Specific discussion about rescaling}

\paragraph{other signatures who tried this in ATLAS and CMS}

\subsubsection{Final proposal}

\paragraph{a two-dimensional scan in the light pseudoscalar mass (ma) -
heavy pseudoscalar mass (mA) plane where ma = mA, fixing tanBeta to 1.0,
sinTheta to 0.35 and the Dark Matter mass (mDM) to 10 GeV. {[}2{]}}

\paragraph{a one-dimensional scan in DM mass from 1 GeV to 500 GeV for a
point in the middle of the sensitivity range for the mono-V analyses at
mA=600, ma=250 GeV, so the connection between this model and cosmology
is clear as the measured relic density starts being satisfied at values
of DM mass around 100 GeV {[}3{]}}

\paragraph{In order to explore changes in complementarity with different
analyses and kinematics, this should be complemented by:}

\paragraph{a two-dimensional scan in the ma − tanBeta plane, for
comparison with the ttbar+MET/bbar+MET analyses. In this case, the
charged Higgs mass (mH+/-), the heavy pseudoscalar mass (mA) and the
heavy Higgs mass (mH) should be fixed to 600 GeV. {[}2{]}}

\paragraph{two one-dimensional scans in sinTheta for the comparison of
mono-Higgs and bbar+MET analysis (it is expected that the bbar+MET
analysis will only have to rescale previous models/cross-sections)
{[}2{]}: - mH± = mA = mH = 600GeV , ma = 200GeV, tanBeta=1 - mH± = mA =
mH = 1000GeV , ma = 350GeV, tanBeta=1}

\paragraph{The PDF recommended is 5-flavor, unless otherwise discussed
and agreed for individual analyses {[}4{]}. ATLAS will use the NNPDF3.0
PDF set.}

\subsection{Relic density}

\subsubsection{Calculation of relic density for the relic-aware grid}

\subsection{DD/ID comparison (TBC)}

\subsubsection{DD: it's a pseudoscalar}

\subsubsection{ID: possible studies}

\end{document}

\subsection{- Misc info}

\subsubsection{Dear DMWG contributors, Following the discussion at our
previous meetings {[}1{]} and on this mailing list, we would like to
finalize the parameter scan for the 2HDM signals to be used in upcoming
ATLAS and CMS searches. The current proposal is to have: - a
two-dimensional scan in the light pseudoscalar mass (ma) - heavy
pseudoscalar mass (mA) plane where ma = mA, fixing tanBeta to 1.0,
sinTheta to 0.35 and the Dark Matter mass (mDM) to 10 GeV. {[}2{]} - a
one-dimensional scan in DM mass from 1 GeV to 500 GeV for a point in the
middle of the sensitivity range for the mono-V analyses at mA=600,
ma=250 GeV, so the connection between this model and cosmology is clear
as the measured relic density starts being satisfied at values of DM
mass around 100 GeV {[}3{]} In order to explore changes in
complementarity with different analyses and kinematics, this should be
complemented by: - a two-dimensional scan in the ma − tanBeta plane, for
comparison with the ttbar+MET/bbar+MET analyses. In this case, the
charged Higgs mass (mH+/-), the heavy pseudoscalar mass (mA) and the
heavy Higgs mass (mH) should be fixed to 600 GeV. {[}2{]} - two
one-dimensional scans in sinTheta for the comparison of mono-Higgs and
bbar+MET analysis (it is expected that the bbar+MET analysis will only
have to rescale previous models/cross-sections) {[}2{]}: - mH± = mA = mH
= 600GeV , ma = 200GeV, tanBeta=1 - mH± = mA = mH = 1000GeV , ma =
350GeV, tanBeta=1 The granularity of the scan can be chosen by the
experiments and search, and we will follow up with studies and plots
from the various ATLAS and CMS analyses over the course of the write-up.
In all scenarios, the lambda parameters are fixed to 3 to ensure vacuum
stability. The PDF recommended is 5-flavor, unless otherwise discussed
and agreed for individual analyses {[}4{]}. ATLAS will use the NNPDF3.0
PDF set. For all MET-X analyses, please use the *new* UFO in the DMWG
model repository {[}5{]}, which fixes an issue with the heavy flavor
models, as in Uli's previous email {[}6{]}. For the model to be used for
the calculation of the relic density, see the README.txt in the DMWG
model repository {[}4{]}. We welcome analysers to upload the
cross-sections that are found with this parameter scan to the DMWG
cross-section repository {[}7{]} via a git pull request. This proposal
will form the basis for the arXiv write-up concluding this focused
effort. We have already informally contacted some of you for
contributions, and we will send more information about that in the near
future. Let us know if you have not yet been contacted and would like to
contribute. Please let us know your thoughts, further studies or
corrections by September 20th, after that we will consider this proposal
finalized. Thanks, The DMWG organizers {[}1{]} For all meeting minutes,
see: http://lpcc.web.cern.ch/content/dark-matter-wg-documents. For
meeting material, see: https://indico.cern.ch/category/7314/ {[}2{]}
http://cern.ch/go/7Vkj and
https://indico.cern.ch/event/646857/attachments/1479901/2328818/2017\_07\_14\_2HDMa\_hMET\_ATLAS\_finalgrid.pdf
{[}3{]} http://cern.ch/go/7Vkj {[}4{]}
https://gitlab.cern.ch/lhc-dmwg-material/model-repository/tree/master/models/Pseudoscalar\_2HDM
{[}5{]} http://cern.ch/go/ncC7 {[}6{]} http://cern.ch/go/R6GT {[}7{]}
https://gitlab.cern.ch/lhc-dmwg-material/cross\_section-repository}

\subsubsection{There has been some additional discussion on ensuring
that the tanBeta scan mentioned below includes high-tanBeta values. A
few folks reached out to Uli with questions on the details in 1701.07427
. A concrete proposal emerged from that discussion: * Include
high-tan(beta) points to the scan: 50, 45, 40, 35, 30, 25, 20, 15, 10, 5
for M(a) masses between 10 and 350 GeV. The high-tanBeta points would be
of primary interest to the HF + DM searches. Uli's studies have shown
that one can simply reweight the existing tt+DM/bb+DM models from DMF to
the new 2HDM+PS cross sections; full simulation of the newly proposed
2HDM+PS points is not required.}

\subsubsection{{[}4{]} should be: Dear Priscilla, all, regarding the
choice of scheme, 4F or 5F, I would like to state my personal opinion.
Considering that predictions are needed for a search (not a SM
measurement) and this search is made through simplified models (not a
fully fledged DM candidate model), I think it is appropriate to keep
simplicity/easiness of reproducibility among the most important
requirements. Including NLO QCD corrections is also important for both
accuracy and precision and choosing a scheme where these are simple to
compute is useful/handy. I would therefore advise to generically use a
5F scheme, *whenever possible/meaningful*. This will make predictions
simpler to generate and use, and also easier to go to NLO in QCD. The
proviso in the asterisks is to keep in mind that if the scalar mediator
mass starts to be comparable to that of the bottom quark (let's say
below 20-30 GeV), a 4F might become more appropriate (even though might
not be necessary if a large mET is required and the scalar is therefore
boosted, in which case a 5F will be still fine). I think that in case of
doubt a check between predictions obtained in the 4F and 5F could be
useful, even though one has to exercise *caution*, as this is not really
straightforward to make such a comparison (even at LO) and to interpret
it. For this reason, I would not advise (at least at this stage) you to
use the 4F and 5F differences as a systematic uncertainty to be added to
the TH predictions, and just go with usual scale/PDF uncertainties.
Cheers Fabio On 20 Jul 2017, at 11:44, Priscilla Pani wrote: Dear
experts and DMWG contributors, since there are a number of parallel
treads discussing the most appropriate flavour scheme to use for the
2HDM generation, I would like to converge the discussion in a single
place. My take on the issue is that the flavour scheme is important for
b-initiated processes and that the default choice for the other
processes is usually a 5-flavour scheme. This would include mono-H/Z,
mono-jet and DM+tt. Would you agree with this choice? Again my take is
that for the 2HDM+a model, the only process where b-initiated diagrams
have a relevant impact is bb+chichi (unless you go to very high tan-beta
values). For this we either make a complete study and use the difference
as uncertainty (see Uli's extensive explanation below) or make a choice
and clearly state it, since both 4f and 5f schemes have pros and cons
for this process. I would personally also for DM+bb use the 5F choice
for the following reasons: 1) consistency with the other final states,
since otherwise an appropriate 4F NLO UFO, compatible with the 4F
assumptions, need to be provided and used. 2) the scale of the process
will be driven by ma \textgreater{} 150 GeV, which should be high enough
to be in the "high-Higgs masses" regime discussed below where the
Santander matching agrees better with the 5F scheme. Assigning the
difference between 4 and 5 flavour estimate as uncertainty on top of
this choice would be my preferred solution. Finally, for NLO generation
we would like to confirm the following PDF choice: 4F:
NNPDF30\_nlo\_as\_0118\_nf\_4 5F: NNPDF30\_nlo\_as\_0118 Please let me
know which are your thoughts on this and many thanks in advance,
Priscilla Uli Haisch email extract 27/04/2017 some answers inline 1.
regarding your recommendation of using the 5F scheme - can you to please
explain why this is the correct thing to do in both cases? (DM+bb,
DM+tt) + like in tt+Higgs i would calculate tt+MET in the 5F scheme.
this is the obvious choice since all scales involved in the process are
much larger than the bottom mass and thus the b can be treated as
effectively massless as for all processes that feature b quarks at the
level of the hard-scattering process, there are two viable approaches to
compute the bb+Higgs or bb+MET cross section. in the 4F scheme, b quarks
are treated as massive particles, hence no b can appear in the initial
state of the partonic scattering process. this is relevant for those
cases where the physical mass of the b quark is considered as a hard
scale and implies that observables with tagged final-state b quarks are
well defined (and thus can b computed reliably). at any order in
perturbation theory the 4F scheme involves terms log(m\_b/Q) where Q is
the characteristic scale of the g -\textgreater{} bb splitting needed to
produce the final state b's. These logarithmically enhanced terms remain
small as long as Q = O(m\_b), but can spoil the perturbative convergence
when Q \textgreater{}\textgreater{} m\_b. such terms are generally dealt
with by reorganising the perturbative series while resuming them the
logarithms to all orders in alpha\_s. this is precisely achieved by
working in the second viable approaches to compute the bb+Higgs or
bb+MET cross section, i.e. the 5F scheme, which is particularly
important when the characteristic of an observable is that of being
dominated by such logarithms. in this scheme, one assumes massless b
quarks at the level of the short-distance cross section, which are
therefore treated at equal footing as the other light quarks and may
appear as initial state particles. the potentially large logarithms are
effectively resummed through the DGLAP evolution of the b-quark PDFs it
is clear from the discussion above that 4F scheme computations do not
account for logarithmic terms beyond the first few, while 5F scheme
results lack power-suppressed terms (m\_b/Q)\^{}n since m\_b = 0 in the
5F scheme. if either of these properties is important the other scheme
must be preferred. being highly observable dependent, at least for
inclusive quantities neither resummation nor mass effects are dominant
and the two approaches lead to generally similar results. for inclusive
observables the 5F scheme has however the technical advantage of being
much simpler, rendering calculations beyond NLO possible, which is very
hard in the 4F scheme. for more exclusive observables, in particular
regarding the final-state b quarks, the 5F scheme loses its advantage,
because it has compared to the 4F scheme much more limited information
on the final-state kinematics of the bb system. this problem of the 5F
can be alleviated by matching the fixed-order computation to parton
showers (PS) which however only provides leading logarithmic accuracy in
general from the above it should be somewhat obvious that in the case of
bb+Higgs or bb+MET it is not entirely clear whether the 5F or 4F scheme
is preferable. in the case of bb+Higgs this issue has been studied in
detail. see for instance figures 263 and 264 in
https://arxiv.org/pdf/1610.07922.pdf from these figures one can conclude
that for large Higgs masses and sufficiently inclusive measurements the
5F scheme is probably preferable over the 4F scheme since it agrees
better with the Santander matching, FONLL-B and NLO+NNLLpartial+ybyt
results. for small Higgs masses the opposite conclusions can be drawn
and the 4F scheme seems more appropriate i think that the conclusions
that one can draw in the case of the Higgs also apply to the MET case;
if i am correct this means that the best choice of scheme will depend on
how light or heavy the DM mediator is in practice i think one way to
proceed is to generate the signal both in the 5F and 4F scheme and take
the difference to assess the theoretical error; in principle this should
be done at NLO+PS and MadGraph5\_aMC@NLO should be able to do this; one
could also think about implementing a scheme like Santander matching,
FONLL-B and NLO+NNLLpartial+ybyt. personally, i think however that this
is an overkill in the absence of a signal in the bb+MET, tt+MET, t+MET,
etc. channels 2. from your answer I understand it might take time to
update the UFO to be suitable to use at NLO. specifically for the
DM+bb/DM+tt final states - do you have an estimation if running at NLO
is something that might have a large impact on the cross sections and on
the kinematical distributions, comparing to LO? + i think that the K
factors and changes in the distributions are not very big if the
mediator is heavy; in case it is light NLO+PS effects will be important
in particular in the 5F scheme; for the case of a light mediator the
importance of the effects also depends significantly on the experimental
analysis and the cuts that are put on the final-state bottom quarks. i
think a conservative approach would be to proceed as described above and
first generate LO 5F and 4F samples, including also scale variation and
PDF uncertainties into the theory error. at low mediator masses this is
likely to result in a very large error (cf. figure 246 in
https://arxiv.org/pdf/1610.07922.pdf) that will make it difficult to set
bounds on the parameters of the model via bb+MET i hope these
explanations help}
